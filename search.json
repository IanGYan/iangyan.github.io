[{"title":"LLMs应用构建一年之心得（第二篇）","date":"2024-10-05T14:45:00.000Z","url":"/2024/10/05/building-with-llms-part-2/","tags":[["LLM","/tags/LLM/"],["Prompt Engineering","/tags/Prompt-Engineering/"],["RAG","/tags/RAG/"],["Pipeline","/tags/Pipeline/"],["Workflow","/tags/Workflow/"],["Agent","/tags/Agent/"],["Finetune","/tags/Finetune/"]],"categories":[[" ",""]],"content":"作者： Eugene Yan, Bryan Bischof, Charles Frye, Hamel Husain, Jason Liu &amp; Shreya Shankar 原作发布日期：May 31, 2024 翻译：ian 最后修订日期：2024年9月日 有一句伪名人名言如此说道：“业余人士谈论战略和战术。专业人士谈论操作。” —— 江湖传闻此言出自多名领袖。战术视角看到的是一系列独特的难题，而操作视角则看到的是需修复的组织功能障碍的模式。战略视角看到的是机遇，操作视角看到的是值得迎接的挑战。 在本文的第一篇章，我们介绍了使用 LLMs（大型语言模型）的战术细节[1]。在下一部分，我们将放大视野，涵盖长期的战略考虑。而在这一部分，我们将讨论构建LLM应用的操作层面，这些层面介于战略和战术之间，将理论与实践相结合。 运作[2]LLM应用会引发一些与传统软件系统相似的问题，但通常带有新颖的转折，让事情变得有趣。LLM应用带来了全新的问题。我们将这些问题及其答案分为四个部分：数据、模型、产品和人员。 对于数据，我们来解答：你应该如何以及多久审查一次LLM的输入和输出？你如何衡量和减少测试与生产之间的偏差？ 对于模型，我们来解答：你如何将语言模型整合到整个技术栈中？你应该如何考虑模型的版本控制以及在模型和版本之间迁移？ 对于产品，我们来解答：设计应该在应用开发过程中何时介入，为什么是“尽可能早”？你如何设计包含丰富人机交互反馈的用户体验？你如何处理众多相互冲突的需求的来优先度？你如何校准产品风险？ 最后，对于人员，我们来解答：你应该雇佣谁来构建一个成功的LLM应用，以及何时雇佣他们？你如何培养一种实验性的文化？你应如何利用新兴的LLM应用来构建你自己的LLM应用？过程和工具哪个更为关键？ 作为一个AI语言模型：我没有个人主张，因此无法告诉你你提供的这篇介绍是“绝妙还是一般”。但我可以说，此文恰当地为后续内容搭好了台阶。[3] 运作篇：日常事务与组织关注之焦点 数据 正如食材的质量决定菜肴的口味一样，输入数据的质量也制约着机器学习系统的性能。此外，输出数据是判断产品是否正常运作的唯一途径。所有创作者都紧密关注数据，每周花费数小时审视输入和输出数据，以更好地理解数据分布：其模式、临界情况、以及模型的局限性。 检查“开发-生产”之间的偏差 在传统的机器学习Pipeline中，一个常见的错误来源是训练-服务偏差。这种情况发生在训练数据与模型在产品阶段遇到的数据不同的时候。虽然我们可以直接使用LLMs而不进行训练或微调，不需要训练数据集，但类似的“开发-生产”数据偏差引发的问题仍然存在。本质上，我们在开发过程中测试系统所用的数据应该反映系统在产品阶段将面临的数据；否则，我们可能会发现在生产环境中的准确性受到了影响。 LLM的开发-生产偏差可以分为两种类型：结构性的和内容性的。结构性偏差包括了诸如格式差异问题等，例如：一个带有列表类型值的 JSON 字典和一个 JSON 列表之间的差异、大小写不一致、以及拼写错误或句子片段等错误。这些错误可能导致模型性能不可预测，因为不同的LLMs是针对特定的数据格式进行训练的，而提示对微小的变化非常敏感。内容性或“语义”偏差指的是数据表达的含义不同、或所处的上下文语境不同。 与传统的机器学习一样，阶段性评测 LLM 输入/输出对之间的偏差是有用的。简单的指标，如输入和输出的长度、或特定的格式要求（例如 JSON 或 XML），都是跟踪变化的直接手段。对于更“高级”的偏移检测，则可以考虑对输入/输出对的Embedding进行聚类，以检测语义偏移，例如用户讨论主题的变化，可能表明他们在探索模型之前未触碰过的领域。 在测试变化时，例如在提示工程中，要确保保留的数据集是最新的，并反映最新的用户交互类型。例如，如果产品使用过程经常发生拼写错误，那这些错误也应该出现在保留的数据中。除了定量的偏差测量之外，对输出进行定性评估也是有帮助的。应定期审查模型的输出 —— 俗称“情境检查” —— 以确保结果符合预期，且保持了对用户需求的匹配。最后，将非确定性纳入偏差检查也是有用的 —— 可对测试数据集中的每一条输入通过多次运行Pipeline来分析所有输出，从而提升捕捉到那些偶尔才出现的异常的几率。 每天查看 LLM 输入和输出的样本 LLMs正在持续进化，是动态的。尽管它们具有令人印象深刻的零样本(zero-shot)[4]能力，并且其输出往往令人愉悦，但它们出错也极难预测。对于定制任务，定期审查数据样本对于培养对LLMs性能的直观理解至关重要。 来自生产环境的“输入-输出对”是 LLM 应用的&quot;现地现物&quot;（genchi genbutsu[5]），它们是不可替代的。最近的研究强调，随着开发者与更多数据的交互，他们对“好”和“坏”输出的认知会发生变化（即标准漂移）。虽然开发者可以预先制定一些评估 LLM 输出的标准，但这些预定义的标准往往是不完备的。例如，在开发过程中，我们可能会更新提示以增加好的响应的概率并减少坏的响应的概率。这种评估、重新评估和更新标准的迭代过程是必要的，因为不直接观察输出，就很难预测 LLM 的行为或人类的偏好。 为了有效管理这一点，我们应该记录 LLM 的输入和输出。通过每天检查这些日志的样本，可以快速识别并适应新的模式或出错的规律。当发现新问题时，可以立即围绕它编写断言或评估。同样，任何对出错模式的定义的更新都应反映到评估标准中。这些“氛围检查”是产生不良输出的指示信号；可用代码和断言来处理这些信号。最后，这种管理方式须推广开来，例如，将输入和输出的复盘或备注工作添加到每个人的流程中。 与模型协作 通过LLM API，我们可以依赖少数供应商提供的智能能力。虽然这是一个福音，但这些依赖也涉及到性能、延迟、吞吐量和成本的权衡。此外，随着更新、更好的模型不断推出（过去一年几乎每个月都有），我们应该准备好更新我们的产品，淘汰旧模型并迁移到新模型。在这一部分，我们将分享我们在无法完全控制的技术基础上展开工作的经验，也即，当我们无法私有部署和掌控这些模型的时候，如何展开工作。 生成结构化输出以简化下游集成 对于大多数现实世界的用例，LLM的输出将通过某种机器可读的格式被下游应用程序使用。例如，Rechat，一个房地产CRM，需要结构化的响应以便前端渲染部件。同样，Boba，一个用于生成产品战略思路的工具，需要带有标题、摘要、合理性评分和时间范围等字段的结构化输出。还有，LinkedIn分享了关于约束LLM生成YAML的内容，这些YAML用于决定使用的哪项技能，及调用技能的参数。 这种应用模式是Postel定律[6]的一个极端版本：在接受（任意自然语言）时要宽松，在发送（类型化的机器可读对象）时要保守。因此，我们预计这个策略面对变化能够长期保持有效和可靠。[7] 目前，Instructor和Outlines是引导LLM生成结构化输出的事实标准。如果你使用的是LLM API（例如Anthropic, OpenAI），请使用Instructor；如果你使用的是自托管模型（例如Hugging Face），请使用Outlines。 跨模型迁移提示是件麻烦事 有时，我们精心设计的提示在一个模型上表现出色，但在另一个模型上却表现平平。这种情况在我们切换不同模型提供商，或者在同一模型的不同版本之间升级时都可能发生。 举个例子，Voiceflow发现，从gpt-3.5-turbo-0301迁移到gpt-3.5-turbo-1106导致其意图分类任务的性能下降了10%。（幸运的是，他们有评估！）同样，GoDaddy观察到一个向好的趋势，升级到版本1106缩小了gpt-3.5-turbo和gpt-4之间的性能差距。（或者，如果你是个乐观主义者，你可能会对gpt-4在新升级中领先优势没那么大了而感到失望） 因此，如果我们必须跨模型迁移提示，预计这将比简单地更换API端点花费更多时间。不要假设插入相同的提示会导致相似或更好的结果。此外，拥有可靠的自动化评估有助于在迁移前后测量任务性能，并减少手动验证所需的努力。 做好版本控制并锁定模型 在任何机器学习的管道(Pipeline)中，“改变任何东西都会改变一切”。这一点在我们不训练自己的模型，而是依赖（供应商提供的）大语言模型时尤为相关，那些模型可能在我们不知情的情况下发生变化。 幸运的是，许多模型提供商提供了“锁定”指定模型版本（例如，gpt-4-turbo-1106）的选项。这使我们能够使用特定版本的模型权重，确保它们保持不变。在生产中锁定模型版本可以帮助避免模型行为产生意外的变化，这可能导致客户投诉，例如在更换模型后出现的过于冗长的输出或其他未预见的错误。 此外，可以考虑维护一个和生产设置一致但使用最新模型版本的影子管道（shadow pipeline），可以安全地使用新版本进行实验和测试。一旦你验证了这些较新模型的输出稳定性和质量，你就可以自信地在生产环境中更新模型版本。 选择能完成任务的最小模型 在开发新应用时，使用最大、最强大的模型是很诱人的。但一旦我们确定任务在技术上是可行的，就值得实验一下是否可以用较小的模型达到类似的结果。 较小的模型的好处是延迟和成本更低。虽然它可能较弱，但像思维链、n轮提示和上下文学习这样的技术可以帮助较小的模型发挥超出其能力的性能。除了LLM API之外，对我们的特定任务进行微调也可以帮助提高性能。 综合来看，使用较小的模型精心设计的流程往往可以匹配甚至超过单个大模型的输出质量，同时速度更快、成本更低。例如，这篇帖子分享了Haiku + 10-shot提示如何超过zero-shot提示的Opus和GPT-4的轶事。长期来看，我们预计会看到更多使用较小模型的流程工程(flow-engineering)的例子，作为输出质量、延迟和成本的最佳平衡。 再举一个例子，考虑简单的分类任务。像DistilBERT（6700万个参数）这样的轻量级模型是一个出人意料的强大基线。4亿参数的DistilBART是另一个很好的选择 —— 当在开源数据集上进行微调时，它可以以0.84 ROC-AUC识别幻觉，[8]超过了大多数LLM，而延迟和成本不到5%。 关键点就是，不要忽视较小的模型。虽然很容易对每个问题都使用一个巨大的模型，但通过一些创新和实验，我们通常可以找到一个更高效的解决方案。 产品 虽然新技术带来了新的可能性，但打造优秀产品的基本原则是永远不会过时的。因此，即使我们第一次遇到新问题，我们也不必在产品设计上重新发明轮子。将我们的LLM应用开发建立在坚实的产品基础之上，可以让我们为所服务的人们提供真正的价值。 尽早并经常地引入设计 拥有一个设计师会推动你深入理解和思考如何构建和向用户展示你的产品。我们有时将设计师刻板地视为那些把东西变得漂亮的人。但除了用户界面之外，他们还会重新思考如何改进用户体验，即使这意味着打破现有的规则和范式。 设计师尤其擅长将用户需求重新包装成不同的形态。其中一些形式比其他形式更容易（通过AI）得到解决，因此，它们可能为AI解决方案提供或多或少的机会。与其他许多产品一样，构建AI产品应该围绕着要完成的任务，而不是驱动它们的技术。 应多问问自己：“用户要求这个产品为他们做什么？这个任务是聊天机器人擅长的吗？如果自动完成会怎么样？也许还有其他什么！”考虑现有的设计模式以及它们与要完成的任务之间的关系。这些都是设计师能为你的团队能力增加的无价资产。 为“Human-in-the-Loop”[9]设计你的用户体验 有一种获得高质量的标注的方法是将 HITL 集成到用户体验（UX）中。通过允许用户轻松地提供反馈和更正，我们可以改进即时输出并收集有价值的数据来改进我们的模型。 想象一个用户上传和分类他们产品的电子商务平台。我们可以设计几种不同的用户体验： 用户手动选择正确的产品类别；LLM定期检查新产品并在后台纠正分类错误。 用户不选择任何类别；LLM定期在后台对产品进行分类（可能存在错误）。 LLM实时建议产品类别，用户可以验证并根据需要更新。 虽然所有三种方法都涉及LLM，但它们提供了非常不同的用户体验。第一种方法将初始负担放在用户身上，LLM作为后处理检查。第二种方法不需要用户付出任何努力，但不提供透明度或控制。第三种方法找到了正确的平衡。通过让LLM提前建议类别，我们减少了用户的认知负荷，他们不需要学习我们的分类法来分类他们的产品！同时，通过允许用户审查和编辑建议，他们在产品的分类上有最终决定权，将控制权牢牢掌握在他们手中。第三种方法创建了一个自然的模型改进反馈循环作为模型反馈的奖励机制：好的建议被接受（正面标签），坏的建议被更新（负面之后尾随正面标签）。 这种建议、用户验证和数据收集的模式在几种应用中很常见： 编码助手：用户可以接受建议（强正面），接受并调整建议（正面），或忽略建议（负面） Midjourney：用户可以选择放大并下载图像（强正面），改变图像（正面），或生成一组新的图像（负面） 聊天机器人：用户可以对回复点赞（正面）或踩（负面），或者如果回复真的很糟糕，可以选择重新生成回复（强负面） 反馈可以是显性的也可以是隐性的。显性反馈是用户在产品请求后提供的信息；隐性反馈是我们从用户交互中学习到的信息，而不需要用户故意提供反馈。编码助手和Midjourney是隐性反馈的例子，而点赞和踩是显性反馈。如果我们设计好我们的用户体验，比如编码助手和Midjourney，我们可以收集大量的隐性反馈来改进我们的产品和模型。 无情地优先考虑你的需求层次 当我们考虑将我们的演示投入运行时，我们必须考虑以下要求： 可靠性：99.9%的正常运行时间，遵守结构化输出 无害性：不生成攻击性、NSFW[10]或其他有害内容 事实一致性：忠实于提供的上下文，不编造事实 有用性：与用户的需求和请求相关 可扩展性：延迟服务级别协议，支持的吞吐量 成本：因为我们没有无限的预算 更多：安全性、隐私、公平性、GDPR、DMA等 如果我们试图同时解决所有这些要求，我们将永远无法发布任何东西。因此，我们需要无情地进行优先选择。这意味着要清楚什么是不可商量的（例如，可靠性、无害性），没有这些我们的产品就无法运行或不可行。关键在于找到最小可接受的产品。我们必须接受第一版不会完美，只需不断发布并迭代去改善它。 根据用例校准你的风险承受能力 在决定语言模型和应用程序的审查级别时，要考虑用例和受众。对于提供医疗或财务建议的面向客户的聊天机器人，我们需要非常高的安全性和准确性标准。错误或不良输出可能会造成实际伤害并侵蚀信任。但对于不太关键的应用程序，如推荐系统，或面向内部的应用程序，如内容分类或摘要，过度的严格要求只会减缓进度而不会增加太多价值。 这与最近的a16z报告一致，该报告显示许多公司在内部LLM应用程序上比外部应用程序发展得更快。通过实验使用AI提高内部生产力，组织可以在更受控的环境中学习如何管理风险的同时开始获取价值。然后，随着他们获得信心，他们可以扩展到面向客户的用例。 团队与角色 没有哪个职位的功能是容易定义的，但相比其他的职位，在这个新的领域描述职位则更具挑战性。我们将避免使用交叠的职位标题的文氏图，或职位描述的建议。然而，我们将提出一个新的角色 —— AI工程师 —— 的存在，并讨论它的定位。重要的是，我们将讨论其余的团队以及如何进行职能分配。 关注流程，而非工具 面对新范式，比如大语言模型（LLMs），软件工程师往往倾向于依赖工具。这样一来，我们忽略了工具原本要解决的问题和过程。许多工程师因此承担了不必要的复杂性，这对团队的长期生产力产生了负面影响。 例如，这篇博文探讨了某些工具如何自动为大型语言模型生成提示。它认为（在我看来是正确的），那些不先理解问题解决方法或流程就使用这些工具的工程师，最终会担负不必要的技术债。 除了偶然的复杂性，工具往往还不能满足一些特定领域。例如，现在有一个不断壮大的行业，提供“一站式LLM评估工具”，这些工具提供通用的评估器，用于检测有害性、简洁性、语气等。我们见过许多团队在不深入思考其在一些特定领域的失效状况的情况下就采用了这些工具。相比之下，EvalGen则专注于教导用户如何创建特定领域的评估，通过深入参与每个步骤，从制定标准、标注数据到检查评估，全程引导用户。软件引导用户完成的工作流程如下： Shankar, S., et al. (2024). Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences. Retrieved from  EvalGen引导用户通过一套“最佳实践”来制定LLM评估，即： 域测试的定义（从提示词自动引导）。这些测试可以定义为代码断言或使用LLM作为判断。 测试与人类判断的一致性至关重要，这样用户可以确认测试是否符合指定的标准。 随着系统（提示等）的变化迭代测试。 EvalGen为开发者提供了一个评估构建过程的思路模型，而不将他们锚定在特定的工具上。我们发现，在提供了这个套路之后，AI工程师通常会选择更精简的工具或构建自己的工具。 大语言模型（LLMs）的组成部分远不止提示词编写和评估那么简单，这里无法一一列举。然而，重要的是，AI工程师在采用这些工具之前，应努力理解其背后的流程。 永远保持尝试 机器学习产品与实验密不可分。不仅仅是A/B测试、随机对照试验这类，还包括频繁尝试修改系统中最小的组件并进行离线评估。大家如此热衷于评估，其实并不是为了信任度和信心 —— 而是为了推动实验！评估做得越好，实验迭代就越快，从而能更快地找到系统的最佳版本。 现在尝试不同方法解决同一个问题很常见，因为实验成本很低。收集数据和训练模型的成本大大降低——提示工程的成本几乎只相当于人力时间。让团队中的每个人都掌握提示工程的基础知识，这样能鼓励大家进行实验，从而在整个组织中产生多样化的想法。 另外，不要只为了探索而实验 —— 也要利用它们来发挥优势！有了新任务的工作版本？考虑让团队中的其他人以不同的方式处理它，尝试另一种更快的方法，研究像思维链或few-shot这样的提示技巧，以提高质量。不要让你的工具限制了你的实验；如果有，那就重构它，或者买个更好的。 最后，在产品阶段或项目规划阶段，要预留时间进行评估构建和多次实验。可以将其视为工程产品的规格说明，但要在此基础上增加明确的评估标准。在制定路线图时，不要低估实验所需的时间 —— 在获得生产许可之前，需要进行多次开发和评估的迭代，应将此纳入预期。 鼓励每个人使用新的AI技术 随着生成式AI的普及，我们需要整个团队 —— 不仅仅是专家 —— 都能理解和感受到使用这项新技术的力量。没有什么比亲自使用更能培养对大型语言模型（如延迟、故障模式、用户体验）工作原理的直观感受了。大型语言模型相对容易上手：你不需要懂得编程就能优化一个流程的性能，每个人都可以通过提示工程和评估来开始贡献。 教育培训在这其中占据了很大一部分。可以从简单的提示工程基础开始，比如使用n-shot提示和思维链（CoT）等技巧来引导模型输出期望的结果。具备更多知识的人还可以传授一些更技术性的内容，比如大语言模型（LLM）本质上是自回归的。换句话说，虽然输入的词元是并行处理的，但输出的词元是按顺序生成的。因此，延迟更多取决于输出长度而非输入长度 —— 这在设计用户体验和设定性能预期时是一个关键考虑因素。 我们还可以更进一步，提供动手实验和探索的机会。比如举办一个黑客松？虽然让整个团队花几天时间在模拟项目上可能看起来成本高昂，但结果可能会让你大吃一惊。我们知道有一个团队通过黑客松，在一年内加速并几乎完成了他们原本三年的路线图。还有个团队的黑客松，因LLMs则带来了用户体验上的全新颠覆，这种全新的用户体验范式被列为了今年及以后的重点。 别掉进“AI工程就是一切”的陷阱 随着新职位的涌现，起初人们往往会夸大这些角色的能力。这通常会导致一个痛苦的修正过程，因为这些工作的实际范围变得清晰。无论是新入行者还是招聘经理，都可能做出夸大的声明或抱有不切实际的期望。过去十年中，有几个著名的案例，请参考： Data scientist: “someone who is better at statistics than any software engineer and better at software engineering than any statistician” Machine learning engineer (MLE): a software engineering-centric view of machine learning 起初，许多人以为数据科学家单独就能搞定数据驱动项目。然而，后来大家明白过来，数据科学家得跟软件工程师和数据工程师联手，才能高效地开发和部署数据产品。 这种误解在新兴的AI工程师角色中再次出现，有些团队认为AI工程师就是一切。实际上，构建机器学习或AI产品需要广泛的专业角色。我们与十几家公司就AI产品进行了咨询，并持续观察到他们陷入了“AI工程就是一切”的陷阱。因此，实际产品往往难以超越演示阶段，因为公司在构建产品时忽略了关键方面。 例如，评估和衡量对于将产品扩展到超越直觉检查的阶段至关重要。有效评估所需的技能与机器学习工程师传统上所具备的一些优势相吻合 —— 一个完全由 AI 工程师组成的团队可能会缺乏这些技能。合著者Hamel Husain在他最近关于检测数据漂移(data drift)和设计特定领域评估的工作中，阐明了这些技能的重要性。 以下是在构建AI产品的过程中，您需要的各种角色的大致安排，以及您何时需要这些角色： 首先，专注于产品构建。这可能包括一名AI工程师，但并非必须。AI工程师在快速原型设计和迭代产品（用户体验、基础架构等）方面非常有价值 接着, 通过仪器化(instrumenting)[11]你的系统并收集数据来建立正确的基石。根据数据的类型和规模，你可能需要平台工程师和/或数据工程师。你还必须拥有查询和分析这些数据的系统，以便调试问题。 然后，你最终会希望优化你的AI系统。这不一定涉及训练模型。基础步骤包括设计指标、构建评估系统、运行实验、优化RAG检索、调试随机系统等。机器学习工程师（MLEs）在这方面非常擅长（尽管AI工程师也能掌握这些技能）。通常，如果你尚未完成必要的前期工作，雇佣一名 MLE 是没有意义的。 除此之外，你始终需要一位领域专家。在小公司，理想情况下这应该是创始团队——而在大公司，产品经理可以扮演这个角色。了解角色发展和时机至关重要。在不恰当的时机招聘人员（例如，过早招聘了机器学习工程师 ）或按错误的顺序构建，都是浪费时间和金钱，还会导致人员流动。此外，在第1至2阶段，定期与机器学习工程师沟通（但不必全职聘用他们）将有助于公司打下正确的基础。 关于作者 Eugene Yan 设计、构建和运营为大规模客户服务的机器学习系统。他目前是亚马逊的高级应用科学家，在亚马逊他构建了服务于全球数百万客户的推荐系统RecSys 2022主题演讲，并应用LLM来更好地服务客户AI Eng Summit 2023主题演讲。此前，他在Lazada（被阿里巴巴收购）和一家医疗科技 A 轮公司领导机器学习工作。他在eugeneyan.com和ApplyingML.com上撰写和讨论有关ML、RecSys、LLM和工程的内容。 Bryan Bischof 是 Hex 的 AI 负责人，他领导着一支工程师团队构建 Magic —— 一个数据科学和分析的Copilot应用。Bryan在数据相关技术栈的各个领域都有工作经验，领导过分析、机器学习工程、数据平台工程和AI工程团队。他在 Blue Bottle Coffee 创建了数据团队，在 Stitch Fix 领导了几个项目，并在 Weights and Biases 构建了数据团队。Bryan 此前与 O’Reilly 合著了《构建产品推荐系统》一书，并在罗格斯大学研究生院教授数据科学和分析。他的博士学位是纯数学。 Charles Frye 教人们构建AI应用。在发表了精神药理学和神经生物学的研究成果后，他在加州大学伯克利分校获得了博士学位，研究方向为神经网络优化。他已经教授了数千人 AI 应用开发的全栈知识，从线性代数基础到GPU的深奥知识，以及构建可防御的业务，并在Weights and Biases、Full Stack Deep Learning 和 Modal 从事教育和咨询工作。 Hamel Husain 是一位拥有超过25年经验的机器学习工程师。他曾在 Airbnb 和 GitHub 等创新公司工作，其中包括一些早期的LLM研究被 OpenAI 采用，用于代码理解。他还领导并贡献了许多流行的开源机器学习工具。Hamel 目前是一名独立顾问，帮助公司将大型语言模型（LLMs）投入运营，以加速他们的AI产品开发进程。 Jason Liu 是一位杰出的机器学习顾问，以领导团队成功交付AI产品而闻名。Jason 的技术专长涵盖个性化算法、搜索优化、合成数据生成和 MLOps 系统。他的经验包括在 Stitch Fix 等公司工作，在那里他创建了一个推荐框架和可观测性工具，每天处理3.5亿次请求。他还在Meta、纽约大学以及 Limitless AI 和 Trunk Tools 等初创公司担任过其他职务。 Shreya Shankar 是加州大学伯克利分校的机器学习工程师和计算机科学博士生。她是两家初创公司的第一位机器学习工程师，从零开始构建每日服务于数千用户的AI驱动产品。作为研究员，她的工作专注于通过以人为中心的方法解决生产环境中机器学习系统的数据挑战。她的研究成果已在VLDB、SIGMOD、CIDR和CSCW等顶级数据管理和人机交互会议上发表。 联系我们 我们非常希望收到您对这篇文章的想法。您可以通过 contact@applied-llms.org 与我们联系。我们中的许多人都愿意提供各种形式的咨询和建议。如果合适的话，我们会在您与我们联系后将您转介给相应的专家。 致谢 这个系列文章始于一次群聊中的对话，Bryan 开玩笑说他受到启发要写&quot;一年的 AI 工程&quot;。然后，✨魔法✨在群聊中发生了，我们都受到启发，决定一起贡献并分享我们迄今为止所学到的东西。 作者们感谢 Eugene 领导了文档整合和整体结构的大部分工作，以及大部分课程内容。此外，还要感谢他承担主要编辑责任和文档方向。作者们感谢 Bryan 提供了促成这篇文章的灵感，将文章重组为战术、运营和战略部分及其介绍，并推动我们思考如何更好地接触和帮助社区。作者们感谢 Charles 对成本和 LLMOps 的深入探讨，以及将课程编织得更加连贯和紧凑 —— 你应该感谢他让这篇文章只有30页而不是40页！作者们感谢 Hamel 和 Jason 从客户咨询和前线工作中获得的洞见，从客户那里学到的广泛可推广的知识，以及对工具的深入了解。最后，感谢 Shreya 提醒我们评估和严格生产实践的重要性，并将她的研究和原创成果带入这篇文章。 最后，作者们感谢所有在文章中慷慨分享挑战和经验教训的团队，我们在整个系列中都引用了这些内容，同时也感谢 AI 社区对本团队的积极参与和互动。 原文链接 译注 “tactical nuts and bolts of working with LLMs” 指的是使用大型语言模型（LLMs）时需要掌握的具体操作和细节。“nubs and bolts”原为机械装置的螺母与螺帽等基本零件，引申为问题的关键点和细节，是一个英语常用短语。 ↩︎ “Operation/Operating”在中文语境中，有多种翻译，包括：操作，运营等。考虑到“运营”在中文语境中，一般特指经营一个系统或平台软件，而“操作”则更侧重于执行具体任务。而本文的内容涵盖了从数据准备到人员团队的与LLMs相关的全生命周期，因此，本文根据上下文语境，将之翻译为“运作”。 ↩︎ 此处，作者模拟一个大语言模型的口吻来评价这篇文章，揭示了这篇文章是为下一篇文章作出了铺垫。 ↩︎ &quot;Zero-shot&quot;是一种机器学习中的技术，意思是让计算机在没有见过某个特定例子的情况下，也能理解和处理这个例子。 ↩︎ “Genchi genbutsu” 的日语写法是「現地現物（ゲンチ ゲンブツ）」，意思是：当你遇到问题时，不要只坐在办公室里猜测或听别人说，而是要亲自去问题的发生地点，亲眼看看实际情况，这样才能真正了解问题，找到最好的解决办法，通常用于强调在解决问题或做出决策时，亲自到现场去观察实际情况的重要性。 ↩︎ Postel定律，也称为鲁棒性原则（Robustness Principle），是由互联网早期先驱Jon Postel提出的一个软件设计指南。该定律的核心思想是：“在你发送的内容上要保守，在你接收的内容上要开放。” 具体来说，这意味着软件在发送数据时应严格遵循协议规范，而在接收数据时则应尽可能宽容，即使数据不完全符合规范也应尝试处理。Postel定律的主要目的是提高系统的鲁棒性和兼容性，使得不同系统之间的通信更加稳定和可靠。然而，这一原则也有其争议，特别是在安全性和协议僵化（Protocol Ossification）方面。一些人认为，过于宽容的接收可能会导致安全漏洞，因为系统可能会无意中处理恶意数据。Postel定律是一个在软件设计中平衡兼容性和安全性的重要原则，但在实际应用中需要谨慎权衡其利弊。 ↩︎ 此处为意译。原文是：As such, we expect it to be extremely durable。 ↩︎ ROC-AUC（Receiver Operating Characteristic - Area Under Curve）是一种用于评估分类模型性能的指标。具体来说： ROC曲线：ROC曲线是根据不同的分类阈值绘制的，横轴表示假阳性率（False Positive Rate, FPR），纵轴表示真阳性率（True Positive Rate, TPR）。ROC曲线能够直观地展示模型在不同阈值下的分类性能。 AUC值：AUC（Area Under Curve）是ROC曲线下的面积，范围在0到1之间。AUC值越大，表示模型的分类性能越好。AUC值为1表示模型完美分类，而0.5表示模型性能等同于随机猜测。 ↩︎ “Human-in-the-Loop”是指一种在系统或流程中，人类参与其中并发挥重要作用的模式。更细致的解释是，这种模式强调在自动化或智能化的过程中，人类的干预、判断和决策对于优化系统性能、提高结果准确性和确保符合伦理道德等方面具有关键意义。人类并非完全被排除在流程之外，而是与技术相互协作，共同实现目标。 可以翻译为“人在环”或“回路中的人”。译者认为保留英语原味妥妥，并可以将“HITL”作为一个术语使用。 ↩︎ Not safe for work 的缩写，字面意思是不适合在上班的时候看。对于一些涉及色情，暴力等内容，发帖子的人就会在标题中加一个NSFW的标签，以提醒读者注意。 ↩︎ &quot;Instrumenting&quot;是指在软件系统中插入代码，以便收集有关系统运行状况和性能的数据。通过仪器化，开发人员可以监视系统的运行情况，识别潜在的性能瓶颈，优化系统性能，提高系统的稳定性和可靠性。仪器化是软件开发和运维中的重要实践，有助于开发人员更好地了解系统的运行情况，及时发现和解决问题，提高系统的性能和可维护性。 ↩︎ "},{"title":"LLMs应用构建一年之心得（第一篇）","date":"2024-09-08T03:10:31.000Z","url":"/2024/09/08/building-with-llms-part-1/","tags":[["LLM","/tags/LLM/"],["Prompt Engineering","/tags/Prompt-Engineering/"],["RAG","/tags/RAG/"],["Pipeline","/tags/Pipeline/"],["Workflow","/tags/Workflow/"],["Agent","/tags/Agent/"],["Finetune","/tags/Finetune/"]],"categories":[[" ",""]],"content":"作者： Eugene Yan, Bryan Bischof, Charles Frye, Hamel Husain, Jason Liu &amp; Shreya Shankar 翻译：ian 原作发布日期：May 28, 2024 第三次校译稿说明 此文是经验丰富的大语言模型的实践者们毫无保留的分享，具有极高的参考价值。 第一稿译文经 Deepseek V2 翻译，但是大量专业术语翻译有误，语叙不畅，阅读困难。此后，译者于空闲时间逐段阅读，并校正了大部分译文，尽量做到能够让读者在不需要对照原文的情况下，也能较为顺畅地以中文阅读本文。 在第二次校译中，我将原标题做了意译，显得更符合中文的意境表达，也更准确地传达了本文的内容方向。翻译中依然存在一些术语或词汇先后不一致的问题，部分情况下使用了中文，部分情况下使用了英文原词，对于有一定经验的开发者来讲，应该能够加以区分。 在第三次校译过程中，译者又发现了大量翻译不恰当之处以及明显的错误。不过，部分译句因英语习惯与中文习惯的不同而略显生涩，且依然可能存在一些排版错误，如有更正，欢迎评论留言。 译者计划将继续翻译此系列文章的续篇，敬请关注。 阅读建议 这篇文章内容很长，仔细阅读需要花费较长时间。部分段中文字还添加了交叉链接至外部资料。对于有一定经验的开发者，建议浏览标题，快速定位至感兴趣的段落阅读。如果有时间，则可以从头开始细读。原文链接也附在译文末尾，以方便读者参照。 特别感谢 出于知识产权考虑，译者向作者发送了邮件，请求翻译本文并发布于本人的公众号或个人博客，用于与朋友之间的交流与学习。译者几乎立即就得到了多位原作者们的同意，对于作者们无私的分享，在此表示衷心地感谢！ 2024年9月8日 译者：ian (以下为译文) 现在是利用大型语言模型（LLMs）构建应用的激动人心时刻。过去一年里，LLMs已经&quot;足够好&quot;，可用于实际应用。LLMs的快速进步，加上社交媒体上的一系列演示，预计将推动2025年对AI的2000亿美元投资。LLMs的广泛可用性使得不仅是机器学习专家，每个人都能将智能融入产品中。尽管构建AI产品的门槛已降低，但创建超越演示的有效产品仍是一项具有挑战性的任务。 我们发现了一些关键但常被忽视的、源于机器学习的见解和方法，这些对基于LLMs开发产品至关重要。掌握这些概念可以让你在不需要ML专业知识的情况下，在这个领域获得竞争优势！过去一年，我们六人一直在LLMs基础上构建实际应用。我们意识到有必要将这些经验教训汇总，以造福整个社区。 我们来自不同背景，担任各异角色，但都亲身体验了使用这项新技术的挑战。我们中有两位独立顾问，他们帮助众多客户将LLM项目从概念变为成功产品，见证了决定成败的关键因素。一位是研究ML/AI团队工作方式及如何改进工作流的研究员；两位是应用AI团队的领导者：一位在科技巨头公司，另一位在初创公司；还有一位曾教授数千人深度学习，现致力于简化AI工具和基础设施的使用。尽管经历各异，我们都对所学到的教训中的共同主题感到惊讶，更令人意外的是这些见解尚未得到广泛讨论。 我们的目标是将此指南打造成围绕LLMs构建成功产品的实用手册，融合我们的亲身经历并借鉴业内范例。过去一年里，我们通过实践积累了宝贵的经验教训，往往来之不易。尽管我们无意代表整个行业发声，但我们诚挚地分享这些关于利用LLMs开发产品的见解和建议。 本文分为战术、运作和战略三个篇章。这是第一篇章，深入探讨了使用LLMs的战术细节。我们分享了提示（Prompting）、构建增强生成检索（RAG）、流程工程（Flow Engineering）应用以及评估和监控的最佳实践和常见陷阱。无论你是使用LLMs构建产品的实践者还是“周末项目开发”的爱好者 [1] ，这些内容都是为你量身打造。敬请期待未来几周的“运作篇”和“战略篇”。 准备好（入坑 ）了吗？让我们开始吧。 战术篇：LLM应用入微 在此篇中，我们分享了新兴LLM技术栈核心组件的最佳实践：提升质量和可靠性的提示技巧、评估输出的策略、改进检索增强生成的方法等。我们还探讨如何设计人机协作工作流。尽管技术仍在快速发展，我们希望这些源自我们共同进行的无数实验的经验教训能经受时间考验，助你构建并发布强大的LLM应用。 提示 (Prompting) 在开发新应用时，我们建议从提示技巧入手。这种方法既容易被低估，也易被高估。被低估是因为恰当运用正确的提示技巧可以让我们事半功倍。被高估则是因为即便是基于提示的应用，也需要围绕提示进行大量的工程工作才能良好运行。 专注于从基本提示技巧中获得最大收益 几种提示技巧在不同模型和任务中均能提高性能，包括：多轮对话中加入上下文学习、思维链推理，以及提供相关资源。 多轮对话（n-shot prompts）中的上下文学习是指：为LLM提供一些示例，以演示任务并使输出符合我们的预期。以下是一些建议： 如果对话轮次(n)太少，模型可能会过度锚定在这些特定示例上，影响其泛化能力。作为经验法则，目标 n ≥ 5。不要担心高达几十轮的对话。 示例应代表预期的输入分布。例如，如果你正在构建电影摘要工具，应包括不同类型电影的样本示例，且大致按照实际中预期出现的比例来选择。 不必一定提供完整的“输入-输出”对。在很多情况下，给出期望输出的示例就足够了。 如果使用支持工具调用的LLM，你的 n-shot 示例也应包含你希望 Agent [2] 使用的工具。 在思维链（Chain of Thought，CoT）提示中，我们鼓励LLM在给出最终答案前阐述其思考过程。这就像为LLM提供一个草稿板，让它不必全靠内存运作。最初的方法是简单地在指令中加入&quot;让我们逐步思考（Let’s think step-by-step）&quot;这样的短语。不过，我们发现让思维链越具体则越有效 —— 添加一两句详细说明通常能大幅降低幻觉率。比如，当让LLM总结会议记录时，我们可以明确指出步骤，如： 首先，在草稿板上列出关键决策、跟进项和相关负责人。 然后，检查草稿板中的细节是否与记录事实一致。 最后，将关键点汇总成简洁的摘要。 最近，有研究质疑这种技术的实际效果是否如此显著。此外，学界对于思维链在推理过程中的具体作用仍存在广泛争议。尽管如此，这种技术仍值得在适当场景下进行实验和探索。 提供相关资源是一种强大机制，可扩展模型知识库、减少幻觉并增加用户信任。这通常通过检索增强生成（RAG）实现，即向模型提供可直接用于响应的文本片段。然而，仅包含这些资源是不够的。务必指示模型优先使用它们，直接引用它们，并在资源不足时予以说明。这些做法有助于将 Agent 的响应&quot;锚定&quot;到资源语料库中，确保输出的准确性和可靠性。 结构化输入和输出 结构化输入和输出能帮助模型更准确地理解输入内容，并生成可靠地集成到下游系统的输出。为输入添加序列化格式有助于模型更好地把握上下文中各个Token [3] 之间的关系，为特定 Token 提供额外的元数据（如类型），或将当前请求与模型训练数据中的相似示例联系起来。 例如，互联网上许多关于SQL编写的问题都以指定SQL模式开始。因此，你可能会认为有效的 Text-to-SQL 提示应包含结构化模式（structured schema）定义。确实如此，研究证实了这一点。 结构化输出不仅服务于类似目的，还简化了系统下游组件的集成。对于结构化输出，Instructor和Outlines是理想选择。具体而言，使用LLM API SDK时选择Instructor，而自托管模型时则选用Outlines。结构化输入能清晰表达任务需求，其格式与训练数据相似，从而提高了生成高质量输出的可能性。 使用结构化输入时，请注意各 LLM 家族的偏好差异。Claude 倾向于 XML 格式，而 GPT 则青睐 Markdown 和 JSON。值得一提的是，在使用XML时，你还可以通过提供response标签来预先填充Claude的回应内容。 做好一件事且只做一件事的小的提示 软件中常见的反模式的代码异味 [4] 是&quot;上帝对象(God Object),&quot; —— 一个单一的类或函数试图承担所有功能。这个问题同样适用于提示工程。 提示通常始于简单：几句指令和几个示例就足够了。然而，当我们努力提升性能并应对更多边缘情况时，复杂性悄然而至。指令变多了，推理步骤增加了，示例数量激增。不知不觉间，我们最初简洁的提示演变成了一个有2000个 Token 的&quot;怪物&quot;。更糟的是，它在处理常见和直接的输入时反而表现更差！GoDaddy将这一经验总结为他们使用LLMs构建应用时的首要教训。 就像我们努力（或者说挣扎着）保持系统和代码简洁一样，我们也应该为提示做同样的事。与其为会议记录摘要器设计一个包罗万象的提示，不如将其分解为以下步骤： 将关键决策、行动项和负责人提取到结构化格式中 检查提取的细节是否与原始转录一致 从结构化详细信息生成简洁摘要 结果，我们将单个提示拆分为多个简单、专注且易于理解的提示。通过分解，我们现在可以单独迭代并评估每个提示。 精心设计上下文Token 重新审视并质疑你对需要发送给 Agent 的上下文量的假设。像米开朗基罗雕刻大卫像那样，不要堆砌你的上下文，而是精心削去多余部分，直到精华呈现。RAG 是整理所有潜在相关信息的流行方法，但你如何从中提炼出真正必要的内容？ 我们发现，将最终发送给模型的提示（包括所有上下文构建、元提示、和 RAG 检索结果）放在空白页上仔细阅读，确实能帮助你重新审视你的上下文。通过这种方法，我们发现了冗余内容、自相矛盾的表述和不恰当的格式。 另一个关键优化是上下文结构。别以为你那一大堆文档会对 Agent 有用，它们对人类都还没帮上忙呢。好好琢磨怎么构建上下文，突出各部分之间的联系，让信息提取简单到极简。 信息检索/RAG 除提示外，另一种有效引导 LLM 的方法是在提示中提供知识。这让 LLM 能基于给定的上下文进行情境学习，即所谓的&quot;检索增强生成（Retrieval-Augmented Generation, RAG）&quot;。实践表明，RAG不仅能有效提供知识、改善输出，还比微调需要更少的努力和成本。然而，RAG 的效果很大程度上取决于检索文档的相关性、信息密度和细节程度。 RAG 输出的质量取决于文档检索的质量，这又可以考虑几个因素 第一个也是最明显的指标是相关性。这通常通过排名指标来量化，如平均倒数排名（MRR）或归一化折损累积增益（NDCG）。MRR 评估的是相关结果放在排名列表中的第一个位置的效果，而 NDCG 则考虑所有结果的相关性及其位置。它们衡量系统在将相关文档排名更高、无关文档排名更低方面的表现。例如，如果我们正在检索用户摘要以生成电影评论摘要，我们会希望将特定电影的评论排名更高，同时排除其他电影的评论。 与传统推荐系统类似，检索项的排名对 LLM 在下游任务中的表现有重大影响。要衡量这一影响，可以尝试以下方法：运行基于 RAG 的任务，但将检索项随机打乱顺序，然后观察 RAG 输出的表现变化。 其次，我们还需要考虑信息密度。如果两个文档同样相关，我们应该优先选择更简洁、冗余信息较少的文档。回到我们的电影例子，从广义上讲，我们可能认为电影剧本和所有用户评论都是相关的。然而，高评分评论和专业评论可能会包含更高密度的信息。 最后，考虑文档提供的详细程度。想象我们正在构建一个从自然语言生成 SQL 查询的 RAG 系统。我们可以简单地提供带有列名的表结构作为上下文。但是，如果我们包括了列的描述和一些代表性值呢？这些额外的细节可以帮助LLM更好地理解表的语义，从而生成更准确的 SQL。 不要忘记关键词搜索；将其作为基线和混合搜索使用 考虑到基于 Embedding [5] 的 RAG 演示如此普遍，人们很容易忘记或忽视信息检索领域几十年来的研究和解决方案。 尽管 Embedding 无疑是一个强大的工具，但它们并非万能良药。首先，虽然它们擅长捕捉高层次的语义相似性，但在处理更具体的、基于关键词的查询时可能会遇到困难，比如当用户搜索名称（如 Ilya）、缩写词（如 RAG）或 ID（如 claude-3-sonnet）时。基于关键词的搜索，如 BM25，正是为此而设计的。经过多年的基于关键词的搜索使用，用户可能已经习以为常，如果他们期望检索的文档没有被返回，他们可能会感到沮丧。 向量Embedding 并不能 神奇地解决搜索问题。事实上，在使用语义相似度搜索进行重新排序之前的步骤才是最重要的。要在 BM25 或全文搜索的基础上实现真正的改进是很困难的。 — Aravind Srinivas, Perplexity.ai CEO 我们已经向客户和合作伙伴传达这一信息数月之久。使用简单 Embedding 的最近邻搜索会产生非常嘈杂的结果，你很可能从基于关键词的方法入手会更好。 — Beyang Liu, Sourcegraph CTO 第二，使用关键词搜索更容易理解为什么会检索到某个文档 —— 我们可以查看与查询匹配的关键词。相比之下，基于 Embedding 的检索可解释性较差。得益于像 Lucene 和 OpenSearch 这样经过数十年优化和实战检验的系统，关键词搜索最终在计算效率上通常更高。 在大多数情况下，混合方法效果最佳：关键词匹配用于明显的匹配项，而 Embedding 则用于同义词、上位词和拼写错误，以及多模态（例如图像和文本）。Shortwave分享了他们如何构建 RAG pipeline [6]，包括查询重写、关键词 + Embedding 检索和排名。 RAG优先，而非微调以获取新知识 RAG 和微调都可以用来将新信息整合到 LLM 中，并提高其在特定任务上的表现。那么，我们应该先尝试哪一种方法呢？ 最新研究表明 RAG 可能更具优势。一项研究比较了 RAG 与无监督微调（又称&quot;继续预训练&quot;），在 MMLU 的一个子集和当下的事件上对两者进行了评估。他们发现，无论是对训练期间遇到的知识还是全新知识，RAG 都始终优于微调。在另一篇论文中，他们在一个农业数据集上比较了 RAG 与监督微调。同样，RAG 带来的性能提升大于微调，尤其是对于 GPT-4（参见论文中的表20）。 除了性能提升，RAG 还具有几个实际优势。首先，与&quot;继续预训练&quot;或&quot;微调&quot;相比，保持检索索引更新更容易，且成本更低。其次，如果我们的检索索引中包含存在问题的文档，如含有有害或偏见内容，我们可以轻松删除或修改这些有问题的文档。 此外，RAG 中的&quot;R&quot;提供了更精细的控制，使我们能够控制如何检索文档。例如，如果我们为多个组织托管 RAG 系统，通过对检索索引进行分区，我们可以确保每个组织只能从自己的索引中检索文档。这样可以保证我们不会无意中将一个组织的信息暴露给另一个组织。 长上下文模型不会使 RAG 过时 随着Gemini 1.5提供了高达1000万Token的上下文窗口，一些人开始质疑RAG的未来。 我倾向于认为Gemini 1.5被Sora的过度炒作掩盖了锋芒 [7]。1000万Token的上下文窗口实际上使得大多数现有的RAG框架变得不必要——你只需将你的数据放入上下文中，像往常一样与模型对话。想象一下这对所有那些工程努力主要集中在RAG上的初创公司/Agents/LangChain项目会产生什么影响😅 或者用一句话来说：1000万上下文杀死了 RAG。干得漂亮，Gemini。 — Yao Fu 虽然长上下文确实会改变游戏规则，比如分析多个文档或与PDF聊天，但 RAG 消亡之说是过于夸大了。 首先，即使有1000万Token的上下文窗口，我们仍然需要一种方法来选择信息输入模型。其次，除了&quot;大海捞针评估法&quot;之外，我们还没有看到令人信服的数据表明模型能够有效地对如此大的上下文进行推理。因此，如果没有良好的检索（和排序）机制，我们可能会用干扰信息淹没模型，甚至可能用完全无关的信息填满上下文窗口。 最后，还有成本问题。Transformer 的推理成本随上下文长度呈二次方增长（或在空间和时间上都呈线性增长）。仅仅因为存在一个模型，在回答每个问题前都能阅读你公司内整个 Google Drive 内容，就去训练并实现它，并不意味着是个好主意。做个类比，我们如何使用RAM：我们仍然从磁盘读写，即使存在 RAM 容量达数十 TB 的计算设施。 所以，不要急着把你的 RAG 扔进垃圾桶。即使上下文窗口变大，这种模式仍然有用。 调整和优化工作流 提示LLM只是开始。为了榨取它们的最大价值，我们需要超越单个提示，拥抱工作流。例如，我们如何将一个复杂的任务分解为多个简单的任务？何时微调或缓存有助于提高性能和降低延迟或成本？在本节中，我们分享了经过验证的策略和实际案例，帮助你优化和构建可靠的LLM工作流。 分步、多轮的“流程”可以带来巨大的提升 我们已经知道，通过将一个大的提示分解为多个较小的提示，我们可以获得更好的结果。一个例子是AlphaCodium：通过从单一提示转换为多步骤工作流，他们在CodeContests上将GPT-4的准确率（pass@5）从19%提高到44%。该工作流包括： 反思问题 对公共测试进行推理 生成可能的解决方案 对可能的解决方案进行排序 生成合成测试 在公共和合成测试上迭代解决方案。 具有明确目标的小任务最适合 Agent 或流程的提示。并非每个 Agent 提示都需要请求进行结构化输出，但，在Agent与某负责外部环境交互的系统进行对接时，结构化输出大有助益。 一些尝试： 一个明确的规划步骤，尽可能详细具体。考虑准备好可供选择的预定义的计划（请参考 ）。 将原用户提示重写为 Agent 提示。小心，这个过程是有损的（会造成信息损失）！ 将Agent行为设计为线性链、DAG（有向无环图） 和 状态机；不同的依赖关系和逻辑关系在不同规模下可能更适合或不太适合。是否能在不同的任务架构中找到合适的结构以优化性能？[8] 规划验证；你的规划可以囊括如何评估其他 Agent 的响应，以确保最终整合效果良好。 固定上游状态的提示工程 —— 确保你的Agent提示针对可能发生的前置情况进行评估。 第一时间优先考虑确定性工作流 虽然AI Agent可以动态地响应用户请求和环境，但它们的非确定性本质使得部署成为一项挑战。Agent 每一步都有失败的可能性，而且从错误中恢复的机会很小。因此，随着步骤数量的增加，Agent 成功完成多步骤任务的可能性呈指数级下降。结果就是，构建Agent 的团队发现很难部署可靠的 Agent。 一种可行的办法是让 Agent 系统生成确定性计划，然后以结构化、可重复的方式执行这些计划。在第一步中，给定一个高层次的目标或提示，Agent生成一个计划。然后，该计划被确定性地执行。这使得每一步都更可预测和可靠。好处包括： 生成的计划可以作为少量样本，用于提示或调优 Agent。 确定性执行使系统更可靠，因此更易于测试和调试。此外，失败可以追溯到计划中的具体步骤。 生成的计划可以表示为有向无环图（DAG），相对于静态提示，更容易理解和适应新情况。 最成功的 Agent 构建者可能是那些在管理初级工程师方面拥有丰富经验的人，因为生成计划的过程与我们指导和管理初级工程师的方式相似。我们给初级工程师明确的目标和具体的计划，而不是模糊的开放式指示，我们也应该对我们的 Agent 采取同样的做法。 最终，构建可靠、高效的 Agent 的关键可能在于采用更加结构化、确定性的方法，同时收集数据来优化提示和微调模型。如果不这样做，我们可能会构建出有时表现出色，但平均而言会让用户失望的 Agent，并导致用户流失。 通过温度以外的其他方式获得更多样化的输出 假设你的任务需要 LLM 输出的多样性。也许你正在编写一个LLM Pipeline ，根据用户之前购买的产品列表从你的目录中推荐产品。当多次运行你的提示时，你可能会注意到产生的推荐结果过于相似 —— 因此你可能会增加 LLM 请求中的温度参数。 简而言之，提高温度参数会使 LLM 的响应更加多样化。在采样时，下一个 Token 的概率分布变得更加平坦，这意味着通常不太可能的 Token 被选择的频率更高。然而，在提高温度时，你可能会注意到一些与输出多样性相关的失效模式。例如，目录中一些可能适合的商品或许永远不会被 LLM 输出。如果根据 LLM 训练时学到的内容，某些商品具有高概率会跟随在某提示之后，那么这少数几个商品就可能在输出中被过度引用。而如果温度过高，你可能会得到引用不存在商品的输出（或者是胡言乱语！） 换句话说，提高温度并不能保证 LLM 会从你期望的概率分布（例如均匀随机）中采样输出。尽管如此，我们还有其他技巧来增加输出多样性。最简单的方法是调整提示中的元素。例如，如果提示模板包含一个项目列表，比如历史购买记录，每次将这些项目插入提示时打乱它们的顺序可以产生显著差异。 此外，保留一个最近输出的简短列表可以帮助防止重复。在我们的推荐商品示例中，通过指示 LLM 避免从这个最近列表中推荐商品，或者拒绝并重新采样与最近建议相似的输出，我们可以进一步使响应多样化。另一个有效的策略是改变提示中使用的措辞。例如，加入诸如&quot;选择用户会喜欢经常使用的商品&quot;或&quot;选择用户可能会向朋友推荐的商品&quot;之类的短语可以转移焦点，从而影响推荐商品的多样性。 缓存（Caching）被低估了 缓存通过消除对相同输入重新计算响应的需求，节省了成本并消除了生成延迟。此外，如果一个响应之前已经过审核，我们可以提供这些经过审查的响应，减少提供有害或不当内容的风险。 一种直接的缓存方法是使用正在处理的 Item（条目或项） 的唯一 ID，比如我们在总结新闻文章或产品评论时。当收到请求时，我们可以检查缓存中是否已存在摘要。如果存在，我们可以立即返回；如果不存在，我们就生成、审核并提供它，然后将其存储在缓存中以供未来请求使用。 对于开放的查询，我们可以借鉴搜索领域的技术，搜索领域也利用缓存处理开放式输入。自动完成和拼写纠正等功能也有助于规范化用户输入，从而提高缓存命中率。 何时进行微调 我们可能会遇到一些任务，即使是最巧妙设计的提示也无法胜任。例如，即使经过大量的提示工程，我们的系统可能仍然离返回可靠、高质量的输出还有一段距离。如果是这样，那么可能有必要为你的特定任务对模型进行微调。 成功的案例包括： Honeycomb 的自然语言查询助手：起初，&quot;编程手册&quot;与 n-shot 示例一起在提示中提供，用于上下文学习。虽然这种方法效果不错，但通过微调模型在领域特定语言的语法和规则方面产生了更好的输出。 ReChat‘s Lucy：LLM 需要以一种非常规范的格式生成响应，该格式结合了结构化和非结构化数据，以便前端能正确渲染。微调对于使其始终如一地工作至关重要。 尽管微调可能很有效，但它伴随着巨大的成本。我们必须标注微调数据，对模型进行微调和评估，最终还要自行托管它们。因此，要考虑更高的前期成本是否值得。如果仅通过提示就能达到 90% 的效果，那么微调可能就不值得投资。不过，如果我们决定进行微调，为了降低收集人工标注数据的成本，我们可以生成并在合成数据集上进行微调，或者在开源数据集上进行引导式训练。 评估与监控 评估LLMs可能是一个雷区。LLMs的输入和输出是任意的文本，而我们给它们的任务是多样的。尽管如此，严格和周到的评估至关重要 —— OpenAI 的技术领导者致力于评估并为个别评估提供反馈并非巧合。 评估LLM应用引发了多种定义和简化阐述：它可能只是单元测试，或者更像是可观察性，又或者仅仅是数据科学。我们发现所有这些观点都很有用。在接下来的部分中，我们将分享一些关于构建评估和监控 Pipeline 中重要事项的经验教训。 从实际输入/输出样本创建一些基于断言的单元测试 创建由生产中的输入和输出样本组成的单元测试（即&quot;断言&quot;），并根据至少三个标准提供输出的期望。虽然三个标准可能看起来是任意的，但它是一个具有可操作性的起点；更少可能表明你的任务定义不够充分或过于开放，比如通用聊天机器人。这些单元测试或断言应由 Pipeline 的任何变动来触发，无论是编辑提示、通过 RAG 添加新上下文、还是其他变动。这篇文章有一个基于断言测试的实际用例。 考虑从指定在所有响应中包含或排除的短语或想法的断言开始。还要考虑进行检查，以确保单词、项目或句子数量在一定范围内。对于其他类型的生成，断言可能会有所不同。基于执行的评估（ODEX）是评估代码生成的一种强大方法，该方法通过运行生成的代码以确定运行时状态对用户请求来说是否足够。 例如，如果用户要求一个名为foo的新函数；那么在执行 Agent 生成的代码后，foo应该是可调用的！基于执行的评估的一个挑战是 Agent 代码经常与目标代码略有不同，从而偏离出运行时环境。将断言放宽到任何可行答案都能被满足的绝对最弱假设可能会很有效。 最后，像顾客一样使用你自己的产品（也就是“自行饲喂狗粮”），可以洞察依据真实世界数据运行中的问题。这种方法不仅有助于识别潜在弱点，还提供了一些实际使用的生产样本，并可以转化用于评估。 LLM-as-Judge 有（点儿）作用，但它不是银弹 LLM作为评判者（LLM-as-Judge），即我们使用一个强大的 LLM 来评估其他 LLM 的输出，这种方法受到了一些人的质疑。（我们中的一些人最初也是持强烈怀疑态度的。）尽管如此，当实施得当时，LLM-as-Judge 能够与人类判断达成不错的相关性，并且至少可以帮助我们建立关于新提示或技术表现如何的先验认知。具体来说，在进行成对比较（例如，对照组与实验组）时，LLM-as-Judge 通常能正确判断方向，尽管win/loss的幅度可能会有噪音[9]。 以下是一些充分利用LLM-as-Judge的建议： 使用成对比较：不要让LLM在李克特（Liket）量表上给单个输出打分，而是给它两个选项，让它选择更好的一个。这通常会带来更稳定的结果。 控制位置偏差：所呈现的选项顺序可能会影响LLM的决策。为了缓解这一问题，每次成对比较都进行两次，每次交换一对选项的顺序。只需确保在交换后将win归于正确的选项！ 允许平局：在某些情况下，两个选项可能同样好。因此，允许LLM宣布平局，这样它就不必任意选择一个赢方。 使用思维链（Chain-of-Throught，CoT）：在给出最终偏好之前，要求LLM解释其决定可以提高评估的可靠性。作为奖励，这允许你使用一个较弱但更快的LLM，并且仍然能够取得类似的结果。因为通常这部分 Pipeline 是批处理模式，所以思维链带来的额外延迟不是问题。 控制响应长度：LLM倾向于偏向较长的响应。为了缓解这一问题，请确保响应对在长度上相似。 LLM-as-Judge的一个特别强大的应用是检查新提示策略与回归。如果你跟踪了一组生产结果，有时你可以用新的提示策略重新运行这些生产示例，并使用LLM-as-Judge快速评估新策略可能在哪里受到影响。 这里有一个简单但有效的方法来迭代LLM-as-Judge，我们只需记录LLM响应、评判者的评判（思维链）和最终结果。然后与利益相关者一起审查这些内容，以确定需要改进的地方。经过三次迭代，人类和LLM的一致性从68%提高到了94%！ 然而，LLM-as-Judge并非银弹[10]。语言中存在一些微妙的方面，即使是最强大的模型也无法可靠地评估。此外，我们发现传统分类器和奖励模型能够比LLM-as-Judge达到更高的准确率，而且成本和延迟更低。对于代码生成，LLM-as-Judge可能比基于执行的评估等更直接的评估策略更弱。 针对评估生成的“实习生测试” 我们喜欢在评估生成时使用以下“实习生测试”：如果你把语言模型的确切输入，包括上下文，作为一个任务交给相关专业的普通大学生，他们能成功吗？需要多长时间？ 如果答案是否定的，因为LLM缺乏所需的知识，则考虑有什么方法去丰富上下文。 如果答案是否定的，我们根本无法通过改进上下文来解决它，那么我们可能遇到了一个对现在的 LLM 来说太难的任务。 如果答案是肯定的，但需要一段时间，则我们可以尝试降低任务的复杂性。它是否可分解？任务的哪些方面可以模板化？ 如果答案是肯定的，他们能很快完成，那就该深入研究数据了。模型做错了什么？我们能找到失败的规律吗？尝试在模型回答前后让它解释自己，这有助于你理解它的思维过程。 过度强调某些评估可能会损害整体性能 “当一个衡量标准成为目标时，它就不再是一个好的衡量标准。” —— Goodhart’s Law 古德哈特定律 有个例子叫做“大海捞针”（Needle-in-a-Haystack，简称&quot;NIAH&quot;）评估。最初，这个评估有助于量化模型在上下文大小增长时的召回率，以及信息位置对召回率的影响。然而，它被过度强调了，在Gemini 1.5的报告中，被放在了图1的位置。NIAH 评估在一个长文档中，重复出现保罗·格雷厄姆的论文，并在其中插入一个特定短语（“The special magic {city} number is: {number}”），然后提示模型去回忆这个魔法数字。 虽然一些模型实现了近乎完美的召回率，但 NIAH 是否真正反映了现实应用中所需的推理和召回能力却值得质疑。考虑一个更实际的场景：给定一个小时的会议记录，LLM 能否总结关键决策和下一步行动，并正确地将每个项目归于相关人员？这个任务更贴近现实，不仅超越了简单的记忆，还考虑了解析复杂讨论、识别相关信息和综合总结的能力。 这里有一个实际的NIAH评估示例。使用医生-患者视频通话的记录，向LLM询问患者的用药情况。它还包括一个更具挑战性的NIAH，插入一个关于随机披萨配料的短语，例如&quot;制作完美披萨所需的秘密配料是：浸泡咖啡的枣、柠檬和山羊奶酪。&quot;在药物任务上的召回率约为80%，而在披萨任务上约为30%。 顺便说一下，过度强调NIAH评估可能导致在提取和总结任务上的性能下降。因为这些LLM被微调得过于关注每一句话，它们可能会开始将不相关的细节和干扰项视为重要，从而在最终输出中包含它们（而这本不应该发生！） 这一点对于其他评估和用例也可能是类似的。例如，摘要生成。对事实一致性的强调可能导致摘要不够具体（这样不太可能出现事实不一致的情况），但也可能变得相关性较低。相反，对写作风格和文采的强调可能导致更花哨、更像广告词的语言，这可能引入与事实的不一致。 简化标注为二元任务或成对比较 为模型输出提供开放式反馈或在李克特量表上进行评分在认知上要求很高。由于人类评分者之间的差异，结果收集到的数据更加嘈杂，因此不太有用。一种更有效的方法是简化任务并减轻标注者的认知负担。两种效果良好的方法是二元分类和成对比较。 在二分类任务中，标注员需要对模型的输出做出简单的“是”或“否”判断。他们可能会被问到生成的摘要是否与源文档在事实上一致，或者提出的回答是否相关，或者是否包含有害内容。与李克特量表相比，二分类决策更精确，评分者之间的一致性更高，并且能提高处理速度。这就是&quot;Doordash如何设置他们的标注流程&quot;提到的，他们通过使用一系列“是”或“否”的决策树来标注菜单项的方式来设置他们的标注流程。 在成对比较中，标注者会看到一对模型响应，并被问哪一个更好。因为人类更容易说&quot;A比B好&quot;而不是单独给A或B打分，这导致比李克特量表更快速和可靠的标注。在一次Llama2见面会上，Llama2论文的作者之一Thomas Scialom证实，成对比较比收集监督微调数据（如书面回答）更快更便宜。前者的成本是每单位3.5美元，而后者的成本是每单位25美元。 如果你正在开始编写标注指南，这里有一些来自Google和Bing搜索的参考指南。 （无参考）评估和护栏可以互换使用 护栏有助于捕捉不恰当或有害的内容，而评估则有助于衡量模型输出的质量和准确性。在无参考评估的情况下，它们可以被视为一枚硬币的两面。无参考评估是不依赖&quot;黄金&quot;参考（如人工编写的答案）的评估，可以仅基于输入提示和模型的响应来评估输出质量。 这些评估的例子包括摘要评估，我们只需考虑输入文档来评估摘要的事实一致性和相关性。如果摘要在这些指标上得分较低，我们可以选择不向用户展示它，有效地将评估用作护栏。同样，无参考翻译评估可以在不需要人工翻译参考的情况下评估翻译质量，这也允许我们将其用作护栏。 LLM会在不应该的时候返回输出 使用LLM时的一个关键挑战是，它们经常会在不应该输出的情况下生成内容。这可能导致无害但毫无意义的回应，或者更严重的问题，如有害或危险的内容。例如，当被要求从文档中提取特定属性或元数据时，即使这些值实际上并不存在，LLM 也可能自信地返回值。另外，由于我们在上下文中提供了非英语文档，模型可能会用英语以外的语言回应。 虽然我们可以尝试提示 LLM 返回“不适用”或“未知”响应，但这并不是万无一失的。即使有对数概率[11]可用，它们也是输出质量的糟糕指标。虽然对数概率表示了某个 Token 出现在输出中的可能性，但它们并不一定反映生成文本的正确性。相反，对于那些经过训练来回应查询并生成连贯回答的指令调优模型，对数概率可能并未得到很好的校准。因此，尽管高对数概率可能表明输出流畅且连贯，但这并不意味着它是准确或相关的。 虽然精心设计的提示工程在一定程度上有所帮助，但我们应该辅以强大的护栏来检测和过滤不良输出并重新生成。例如，OpenAI 提供了一个内容审核API，可以识别不安全的回应，如仇恨言论、自残或色情内容。同样，也有许多用于检测个人身份信息（PII）的软件包。护栏的一个优点是它们在很大程度上与具体用例无关，因此可以广泛应用于特定语言的所有输出。此外，通过精确检索，如果没有相关文档，我们的系统可以确定性地回答&quot;我不知道&quot;。 这里的一个推论是，LLM 可能在预期产生输出时无法产生输出。这可能由各种原因引起，从简单的问题（如API提供商的长尾延迟）到更复杂的问题（如输出被内容审核过滤器阻止）。因此，持续记录输入和（可能缺失的）输出对于调试和监控来说很重要。 幻觉是一个顽固的问题 与内容安全或个人身份信息（PII）缺陷相比，后者受到大量关注因此很少发生，而事实不一致问题却顽固地依然存在，更难以检测。它们更为常见，发生率为 5 ~ 10%，根据我们从 LLM 提供商那里了解到的情况，即使在简单的任务如摘要生成中，要将其降低到 2% 以下也具有挑战性。 为了解决这个问题，我们可以结合使用提示工程（生成上游）和事实不一致护栏（生成下游）。对于提示工程，像 CoT 这样的技术通过让 LLM 在最终返回输出之前解释其推理来减少幻觉。然后，我们可以应用事实不一致护栏来评估摘要的事实性，并过滤幻觉或重新生成。在某些情况下，可以确定性地检测幻觉。当使用 RAG 检索资源时，如果输出是结构化的并标识了资源是什么，你应该能够手动验证它们是否来自输入上下文。 关于作者 Eugene Yan 设计、构建和运营为大规模客户服务的机器学习系统。他目前是亚马逊的高级应用科学家，在亚马逊他构建了服务于全球数百万客户的推荐系统RecSys 2022主题演讲，并应用LLM来更好地服务客户AI Eng Summit 2023主题演讲。此前，他在Lazada（被阿里巴巴收购）和一家医疗科技 A 轮公司领导机器学习工作。他在eugeneyan.com和ApplyingML.com上撰写和讨论有关ML、RecSys、LLM和工程的内容。 Bryan Bischof 是 Hex 的 AI 负责人，他领导着一支工程师团队构建 Magic —— 一个数据科学和分析的Copilot应用。Bryan在数据相关技术栈的各个领域都有工作经验，领导过分析、机器学习工程、数据平台工程和AI工程团队。他在 Blue Bottle Coffee 创建了数据团队，在 Stitch Fix 领导了几个项目，并在 Weights and Biases 构建了数据团队。Bryan 此前与 O’Reilly 合著了《构建产品推荐系统》一书，并在罗格斯大学研究生院教授数据科学和分析。他的博士学位是纯数学。 Charles Frye 教人们构建AI应用。在发表了精神药理学和神经生物学的研究成果后，他在加州大学伯克利分校获得了博士学位，研究方向为神经网络优化。他已经教授了数千人 AI 应用开发的全栈知识，从线性代数基础到GPU的深奥知识，以及构建可防御的业务，并在Weights and Biases、Full Stack Deep Learning 和 Modal 从事教育和咨询工作。 Hamel Husain 是一位拥有超过25年经验的机器学习工程师。他曾在 Airbnb 和 GitHub 等创新公司工作，其中包括一些早期的LLM研究被 OpenAI 采用，用于代码理解。他还领导并贡献了许多流行的开源机器学习工具。Hamel 目前是一名独立顾问，帮助公司将大型语言模型（LLMs）投入运营，以加速他们的AI产品开发进程。 Jason Liu 是一位杰出的机器学习顾问，以领导团队成功交付AI产品而闻名。Jason 的技术专长涵盖个性化算法、搜索优化、合成数据生成和 MLOps 系统。他的经验包括在 Stitch Fix 等公司工作，在那里他创建了一个推荐框架和可观测性工具，每天处理3.5亿次请求。他还在Meta、纽约大学以及 Limitless AI 和 Trunk Tools 等初创公司担任过其他职务。 Shreya Shankar 是加州大学伯克利分校的机器学习工程师和计算机科学博士生。她是两家初创公司的第一位机器学习工程师，从零开始构建每日服务于数千用户的AI驱动产品。作为研究员，她的工作专注于通过以人为中心的方法解决生产环境中机器学习系统的数据挑战。她的研究成果已在VLDB、SIGMOD、CIDR和CSCW等顶级数据管理和人机交互会议上发表。 联系我们 我们非常希望收到您对这篇文章的想法。您可以通过 contact@applied-llms.org 与我们联系。我们中的许多人都愿意提供各种形式的咨询和建议。如果合适的话，我们会在您与我们联系后将您转介给相应的专家。 致谢 这个系列文章始于一次群聊中的对话，Bryan 开玩笑说他受到启发要写&quot;一年的 AI 工程&quot;。然后，✨魔法✨在群聊中发生了，我们都受到启发，决定一起贡献并分享我们迄今为止所学到的东西。 作者们感谢 Eugene 领导了文档整合和整体结构的大部分工作，以及大部分课程内容。此外，还要感谢他承担主要编辑责任和文档方向。作者们感谢 Bryan 提供了促成这篇文章的灵感，将文章重组为战术、运营和战略部分及其介绍，并推动我们思考如何更好地接触和帮助社区。作者们感谢 Charles 对成本和 LLMOps 的深入探讨，以及将课程编织得更加连贯和紧凑 —— 你应该感谢他让这篇文章只有30页而不是40页！作者们感谢 Hamel 和 Jason 从客户咨询和前线工作中获得的洞见，从客户那里学到的广泛可推广的知识，以及对工具的深入了解。最后，感谢 Shreya 提醒我们评估和严格生产实践的重要性，并将她的研究和原创成果带入这篇文章。 最后，作者们感谢所有在文章中慷慨分享挑战和经验教训的团队，我们在整个系列中都引用了这些内容，同时也感谢 AI 社区对本团队的积极参与和互动。 原文链接 译注 指利用业余时间开发软件的爱好者。 ↩︎ 一般也译作“智能体”，本文均保留英文术语。 ↩︎ 在大语言模型领域的这个术语&quot;Token&quot;可被译作“词元”，此文保留英文术语，不作中译。 ↩︎ 也就是程序员常说的&quot;Bad smell&quot;。 ↩︎ 一般译作“嵌入”，此文保留英文术语。 ↩︎ pipeline可以被译为“管道”或“流水线”，此文直接保留英文术语。 ↩︎ 此处Yao用错了词，“overhyped”应作“overshadowed”，当时Sora抢在Gemini 1.5之前宣传了其视频效果，抢占了头条位置，掩盖了Gemini的锋芒。 ↩︎ 此处为意译。 线性链（Linear Chains）：就像一条直线的队伍，每个人都要等前面的人做完才能开始。这种结构很简单，适合任务一步接一步的情况。 有向无环图（DAGs）：就像一个迷宫，每个人可以选择不同的路径，但最终都要到达同一个终点。这种结构适合任务之间有多种选择和依赖关系的情况。 状态机（State-Machines）：就像一个机器，根据不同的状态（比如开、关、暂停）来决定下一步做什么。这种结构适合任务需要根据不同状态做出不同反应的情况。 不同的任务结构适合不同的情况，有时候简单的线性链就够了，有时候需要更复杂的DAGs或状态机。优化性能就是要找到最适合当前任务的结构，让任务完成得更快更好。 ↩︎ 这句话的意思是：LLM-as-Judge通常能正确判断哪个模型更好，但是具体好多少可能判断不准确。 在讨论&quot;LLM-as-Judge&quot;（即使用一个强大的语言模型来评估其他语言模型的输出）时，“win/loss”（胜/负）通常指的是在成对比较中，某个模型相对于另一个模型的表现。具体来说： Win: 表示在某个特定的任务或评估标准下，一个模型（例如“treatment”模型）的表现优于另一个模型（例如“control”模型）。 Loss: 表示在同样的任务或评估标准下，一个模型的表现不如另一个模型。 在&quot;LLM-as-Judge&quot;的上下文中，尽管这个系统在判断哪个模型表现更好（即方向）方面通常是准确的，但在量化这种优势的程度（即幅度）时，可能会存在噪声或不准确性。换句话说，系统可以正确地判断出哪个模型更好，但可能无法精确地衡量这种优势有多大。 例如，假设有两个模型 A 和 B，LLM-as-Judge可能会判断出模型A在某个任务上比模型B表现更好（即A“win”），但它可能无法准确地量化这种优势有多大（即“win”的幅度）。这种情况下，&quot;win/loss&quot;的幅度可能会有噪声，意味着评估结果可能不够精确。 总结来说，&quot;win/loss&quot;在这里指的是模型之间的相对表现，而&quot;magnitude of the win/loss&quot;则指的是这种表现的差异程度。 ↩︎ “Silver bullet” 在中文中的翻译通常是“银弹”。这个词汇源自西方文化，原意是指由银制成的子弹，用于对付狼人等超自然生物。在现代语境中，“silver bullet” 被用作隐喻，指的是针对复杂问题的简单、快速且有效的解决办法。 ↩︎ 对数概率是生成每个Token的概率，这些概率并不能很好地反映回答的质量。 ↩︎ "},{"title":"search","date":"2024-09-09T05:37:03.000Z","url":"/search/index.html","categories":[[" ",""]]},{"title":"categories","date":"2024-09-09T05:47:25.000Z","url":"/categories/index.html","categories":[[" ",""]]},{"title":"tags","date":"2024-09-09T05:46:45.000Z","url":"/tags/index.html","categories":[[" ",""]]}]